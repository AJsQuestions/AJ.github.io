---
title: "Trading Bot in a 100 lines of code" 
date: 2024-01-04
url: /q1
tags: ["Python", "Alpaca", "Machine Learning", "Data Science", "Quant Finance"]
author: "AJ"
summary: "I build a trading bot using a simple strategy in a 100 lines of code" 
cover:
    image: "/AJpicture.jpeg"
    relative: true
showToc: true
disableAnchoredHeadings: false
---

# Building a Trade Bot from Scratch
## In a 100 lines of code

The world of financial trading is complex; it requires thorough knowledge of a number of practical aspects of the trade cycle to successfully run a stratgy.

In this project, I tried to make trading as simple as running a cell in a jupyter notebook.

Of course there was systematic thought involved in building the bot, but the idea wasn't to develop a sophisticated strategy, rather, to develop A strategy FAST.

### 1. The Data

But first...

#### 1.1 Alpaca

Alpaca is an API first broker that allows you to trade and maintain your portfolio using their python REST API. I used Alpaca as my broker and obtained API-keys for my paper trading acccount on Alpaca, as with any other SaaS.

#### 1.2 Universe

I chose my universe of assets to be approximately 200 US ETFs chosen from a larger universe of 4000+ listed US ETFs. The criterion of selection was a threshold on the volume weighted average price having a mean of larger than a 100 in the past month. The volume weighted average price is a proxy measure for the liquidity of the asset. I preferred to deal with highly liquid ETFs.

The list of 4000+ tickers was pulled from the data vendor 12Data. They maintain an API endpoint for all 13000+ ETFs listed on all major exchanges in the world!

This is however, a matter of discretion, and not science, and one is free to choose the universe in any other way they desire, without changing the approach. It could be stocks, bonds, options, currency, crypto, or other alternatives.

#### 1.3 OLHCV Data

With our universe determined, I now pull data for the last 8 years in the following manner:

1. Install and import libraries
<pre><code>
import numpy as np
import pandas as pd

from alpaca_trade_api.rest import REST, TimeFrame
from ta import add_all_ta_features
from datetime import datetime, timedelta
from tqdm.auto import tqdm

from sklearn.tree import DecisionTreeClassifier

import warnings
warnings.filterwarnings("ignore")
</code></pre>

2. Declare your Alpaca API Keys
<pre><code>
api = REST(key_id="<YOUR-API-KEY>", secret_key="<YOUR-SECRTE-KEY>", 
                    base_url="https://paper-api.alpaca.markets", api_version='v2')
</code></pre>

3. Import file of tickers in universe (here, ETF universe)
<pre><code>
us_etfs = pd.read_csv("YOUR-UNIVERSE.csv")
</code></pre>

4. Set up data strcutures to store:
    - symbols to buy
    - symbols to sell
    - last close prices of symbols to buy
    - returns of symbols to buy
<pre><code>
symbols_to_buy = []
symbols_to_sell = []
price = pd.DataFrame()
rets = pd.DataFrame()
</code></pre>

5. Determine training period using the datetime module
<pre><code>
current_time = datetime.now()
tomorrow = current_time + timedelta(days=1)
yesterday = current_time - timedelta(days=1)

# end = current_time.strftime('%Y-%m-%d')
end = yesterday.strftime('%Y-%m-%d')
years = 3
start = current_time.replace(year=current_time.year - years).strftime('%Y-%m-%d')
</code></pre>

6. Pull OLHCV bar data from Alpaca API
We do this in a loop but the general format is:
<pre><code>
bars = api.get_bars(<SYMBOL>, TimeFrame.Day, start, end)
    if len(bars) == 0:
        continue
    else:
        df = bars.df
</code></pre>

#### 1.4 Adding Features
With OLHCV data now pulled, we add all possible trechnical indicators from the library TA-lib using their python wrapper. Technical Indicators are derived from price data and serve as additional features to predict price movements.

We do the following next:
1. Store price data of all tickers in the universe in our price dataframe. This will be of use to us later.
<pre><code>
price[symbol] = df['close']
</code></pre>

2. Add all technical indicators to the dataframe with the TA-Lib module. Here, the functions bfill() and ffill() impute NaNs and missing values in the indicators. Of course we could impute these values with the mean or median, or create a probability distribution and sample from it, or even create an ML model to impute missing values. However, for simplicity, we stick to backward and forward filling.
<pre><code>
df = add_all_ta_features(df, open="open", high="high", low="low", close="close", volume="volume").bfill().ffill()
</code></pre>

3. The log returns for each security is calculated and stored for later use. We also define our signal. This is the value that we will try to predict by building an ML model. The hi_lo_signal simply flags the days when the return was positive with a 1, and days where returns were negative remain 0.

df['ret'] = np.log(df.close) - np.log(df.close.shift(1))
rets[symbol] = df['ret']
df['hi_lo_signal'] = np.where(df.ret>0,1,0)
