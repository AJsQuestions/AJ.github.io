[{"content":" In the Spring 2023, I am on leave at the University of California\u0026ndash;San Diego. I will therefore not hold regular office hours. But if you are a graduate student on the job market or at the research stage, and you would like to talk, please email me to set up a zoom call.\n","permalink":"https://pascalmichaillat.org/officehours/","summary":"In the Spring 2023, I am on leave at the University of California\u0026ndash;San Diego. I will therefore not hold regular office hours. But if you are a graduate student on the job market or at the research stage, and you would like to talk, please email me to set up a zoom call.","title":"Office Hours ‚Äì Spring 2023"},{"content":" Paper Abstract: Most governments are mandated to maintain their economies at full employment. We propose that the best marker of full employment is the efficient unemployment rate, u*. We define u* as the unemployment rate that minimizes the nonproductive use of labor\u0026mdash;both jobseeking and recruiting. The nonproductive use of labor is well measured by the number of jobseekers and vacancies, u + v. Through the Beveridge curve, the number of vacancies is inversely related to the number of jobseekers. With such symmetry, the labor market is efficient when there are as many jobseekers as vacancies (u = v), too tight when there are more vacancies than jobseekers (v \u0026gt; u), and too slack when there are more jobseekers than vacancies (u \u0026gt; v). Accordingly, the efficient unemployment rate is the geometric average of the unemployment and vacancy rates: u* = ‚àöuv. We compute u* for the United States between 1930 and 2022. We find that u* averages 4.2% over the period and, for instance, that the US labor market has been over full employment (u \u0026lt; u*) since May 2021.\nFigure 12C: Efficient unemployment rate, u* = ‚àöuv, in the United States, 1930\u0026ndash;2022 Citation: Michaillat, Pascal and Emmanuel Saez. 2022. \u0026ldquo;u* = ‚àöuv.\u0026rdquo; arXiv:2206.13012. https://doi.org/10.48550/arXiv.2206.13012.\n","permalink":"https://pascalmichaillat.org/13/","summary":"Under simple but realistic assumptions, the efficient unemployment rate u* is the geometric average of the unemployment and vacancy rates. In the United States, 1930\u0026ndash;2022, u* averages 4.2%.","title":"u* = ‚àöuv"},{"content":" Paper Abstract: P-hacking occurs when researchers engage in various behaviors that increase their chances of reporting statistically significant results. P-hacking is problematic because it reduces the informativeness of hypothesis tests\u0026mdash;by making significant results much more common than they are supposed to be in the absence of true significance. Despite its prevalence, p-hacking is not taken into account in hypothesis testing theory: the critical values used to determine significance assume no p-hacking. To address this problem, we build a model of p-hacking and use it to construct critical values such that, if these values are used to determine significance, and if researchers adjust their p-hacking behavior to these new significance standards, then significant results occur with the desired frequency. Because such robust critical values allow for p-hacking, they are larger than classical critical values. As an illustration, we calibrate the model with evidence from the social and medical sciences. We find that the robust critical value for any test is the classical critical value for the same test with one fifth of the significance level\u0026mdash;a form of Bonferroni correction. For instance, for a z-test with a significance level of 5%, the robust critical value is 2.31 instead of 1.65 if the test is one-sided and 2.57 instead of 1.96 if the test is two-sided.\nFigure 3: Robust critical values for different parametrizations Citation: McCloskey, Adam and Pascal Michaillat. 2022. \u0026ldquo;Critical Values Robust to P-Hacking.\u0026rdquo; arXiv:2005.04141. https://doi.org/10.48550/arXiv.2005.04141.\n","permalink":"https://pascalmichaillat.org/12/","summary":"This paper develops a model of p-hacking by researchers. It then gives critical values that correct the inflated type 1 error rates caused by p-hacking. For a two-sided z-test with significance level of 5%, the robust critical value is 2.57 (instead of 1.96).","title":"Critical Values Robust to P-Hacking"},{"content":" Paper Online appendix Code \u0026amp; data Abstract: his paper develops a sufficient-statistic formula for the unemployment gap\u0026mdash;the difference between the actual unemployment rate and the efficient unemployment rate. While lowering unemployment puts more people into work, it forces firms to post more vacancies and to devote more resources to recruiting. This unemployment-vacancy tradeoff, governed by the Beveridge curve, determines the efficient unemployment rate. Accordingly, the unemployment gap can be measured from three sufficient statistics: elasticity of the Beveridge curve, social cost of unemployment, and cost of recruiting. Applying this formula to the United States, 1951\u0026ndash;2019, we find that the efficient unemployment rate averages 4.3%, always remains between 3.0% and 5.4%, and has been stable between 3.8% and 4.6% since 1990. As a result, the unemployment gap is countercyclical, reaching 6 percentage points in slumps. The US labor market is therefore generally inefficient and especially inefficiently slack in slumps. In turn, the unemployment gap is a crucial statistic to design labor-market and macroeconomic policies.\nFigure 7B: Dynamics of the share of Better scientists (ùúé) in the tenured population depending on the power of the scientific field. Citation: Michaillat, Pascal and Emmanuel Saez. 2021. \u0026ldquo;Beveridgean Unemployment Gap.\u0026rdquo; Journal of Public Economics Plus 115 (52): 13228\u0026ndash;13233. https://doi.org/10.1016/j.pubecp.2021.100009.\n","permalink":"https://pascalmichaillat.org/9/","summary":"This paper develops a sufficient-statistic formula for the unemployment gap based on the Beveridge curve. The unemployment gap in the United States is positive most of the time and is countercyclical.","title":"Beveridgean Unemployment Gap"},{"content":" Paper Online appendix Phase diagrams Abstract: At the zero lower bound, the New Keynesian model predicts that output and inflation collapse to implausibly low levels and that government spending and forward guidance have implausibly large effects. To resolve these anomalies, we introduce wealth into the utility function; the justification is that wealth is a marker of social status, and people value status. Since people partly save to accrue social status, the Euler equation is modified. As a result, when the marginal utility of wealth is sufficiently large, the dynamical system representing the zero-lower-bound equilibrium transforms from a saddle to a source, which resolves all the anomalies.\nFigure 1: Phase diagrams of the New Keynesian (NK) model and wealth-in-the-utility New Keynesian (WUNK) model, in normal times and at the zero lower bound (ZLB) Citation: Michaillat, Pascal and Emmanuel Saez. 2021. \u0026ldquo;Resolving New Keynesian Anomalies with Wealth in the Utility Function.\u0026rdquo; Review of Economics and Statistics 103 (2): 197\u0026ndash;215. https://doi.org/10.1162/rest_a_00893.\n","permalink":"https://pascalmichaillat.org/11/","summary":"This paper resolves the anomalies of the New Keynesian model at the zero lower bound\u0026mdash;explosive recession, forward guidance puzzle, multiplier puzzle\u0026mdash;by introducing wealth into the utility function.","title":"Resolving New Keynesian Anomalies with Wealth in the Utility Function"},{"content":" Paper Online appendix Significance: It is believed that a lack of experimental evidence (typical in the social sciences) slows but does not prevent the adoption of true theories. We evaluate this belief using a model of scientific research and promotion in which tenured scientists are slightly biased toward tenure candidates with similar beliefs. We find that when a science lacks evidence to discriminate between theories, or when tenure decisions do not rely on available evidence, true theories may not be adopted. The nonadoption of heliocentric theory in the 16th century, the persistence of bloodletting in the 19th century, the nonadoption of underconsumption theory in the early 20th century, and the persistence of radical mastectomy in the 20th century illustrate such risk.\nAbstract: We develop a model describing how false paradigms may persist, hindering scientific progress. The model features two paradigms, one describing reality better than the other. Tenured scientists display homophily: they favor tenure candidates who adhere to their paradigm. As in statistics, power is the probability (absent any bias) of denying tenure to scientists adhering to the false paradigm. The model shows that because of homophily, when power is low, the false paradigm may prevail. Then only an increase in power can ignite convergence to the true paradigm. Historical case studies suggest that low power comes either from lack of empirical evidence, or from reluctance to base tenure decisions on available evidence.\nFigure 1: Dynamics of the share of Better scientists (ùúé) in the tenured population depending on the power of the scientific field. Citation: Akerlof, George A., and Pascal Michaillat. 2018. \u0026ldquo;Persistence of False Paradigms in Low-Power Sciences.\u0026rdquo; Proceedings of the National Academy of Sciences 115 (52): 13228\u0026ndash;13233. https://doi.org/10.1073/pnas.1816454115.\n","permalink":"https://pascalmichaillat.org/10/","summary":"This paper develops a model of science. It shows that due to homophily in tenure decisions, false paradigms may persist when a science has low power. History suggests that low power comes either from lack of evidence, or from reluctance to base tenure decisions on available evidence.","title":"Persistence of False Paradigms in Low-Power Sciences"}]